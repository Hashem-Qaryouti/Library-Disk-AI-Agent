 # Library-Disk-AI-Agent

A conversational AI-powered library/bookstore assistant built with Streamlit for the frontend and FastAPI for the backend. The system leverages LangChain and Ollama to handle queries, execute tools, and manage book orders, inventory, and customer interactions.

---
## ğŸ” Agent Wokflow

![RAG Workflow](assets/flowchart.png)
## Features

- **Chat-based interface** to interact with the library agent.
- **Session management**: start new sessions or load previous ones.
- **Database-backed storage**: all user queries, agent responses, and executed tool calls are stored in SQLite.
- **Book management tools**:
  - Search books by title or author
  - Create customer orders
  - Restock books
  - Update book prices
  - Check order status
  - Inventory summary for low-stock books
- **Agent orchestration**: executes tools based on user prompts.

---

## ğŸ’¡ Example: Streamlit App in Action

Once you run the Streamlit app, youâ€™ll see an interface like this:
![RAG PDF Q&A Example](assets/app_example.png)

Ask questions about your PDF (for example):

## Requirements

- Python 3.10+
- Libraries (can be installed via `requirements.txt`):

```bash
pip install -r requirements.txt
```
---

## Project Structure
```
.
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api.py                  # FastAPI endpoints
â”‚   â”œâ”€â”€ api_helper_functions.py # Agent helper functions (store messages, call agent)
â”‚   â”œâ”€â”€ agent.py                # LLM and tools setup
â”‚   â”œâ”€â”€ db.py                   # Database query and execution functions
â”‚   â””â”€â”€ tools.py                # Bookstore business logic functions
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ bookstore_prompt.txt    # System prompt for agent
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ library.db              # SQLite database
â”‚   â”œâ”€â”€ schema.sql              # DB schema definitions
â”‚   â””â”€â”€ seed.sql                # Seed data for initial testing
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py                  # Streamlit frontend
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore file
â””â”€â”€ README.md                   # Project README
```
---

## Project Setup
### 1. Clone the repository

```bash
git clone https://github.com/Hashem-Qaryouti/llama3-rag-pipeline.git
cd <your-repo-name>
```
### 2. Create and activate a virtual environment
```bash
python -m venv llama_env
```
#### Activate the environment:
- **Windows:**
```bash
llama_env\Scripts\activate
```
- **Linux:**
```bash
source llama_env/bin/activate
```
### 3. Install dependencies
```bash
pip install -r requirements.txt

```

### 4. Add your PDF
Place your PDF file in the `data/` folder.
Update paths.py or the PDF_PATH variable in `ingest_pdf.py` to point to your PDF file.

### 5. Add your LLaMA model

Place your downloaded LLaMA GGUF model in the `models/` folder.
Example:
```bash
models/Dolphin3.0-Llama3.2-1B-Q4_K_M.gguf
```
---

## Usage
1. Run the Streamlit App
```bash
cd src
streamlit run app.py
```
- **Open the URL shown in your terminal (usually http://localhost:8501):**
- **Enter your question in the text box and click Ask**
- **View the answer generated by LLaMA based on your PDF content**
