 # Library-Disk-AI-Agent

A conversational AI-powered library/bookstore assistant built with Streamlit for the frontend and FastAPI for the backend. The system leverages LangChain and Ollama to handle queries, execute tools, and manage book orders, inventory, and customer interactions.

---
## ğŸ” Agent Wokflow

![RAG Workflow](assets/flowchart.png)
## Features

- **Chat-based interface** to interact with the library agent.
- **Session management**: start new sessions or load previous ones.
- **Database-backed storage**: all user queries, agent responses, and executed tool calls are stored in SQLite.
- **Book management tools**:
  - Search books by title or author
  - Create customer orders
  - Restock books
  - Update book prices
  - Check order status
  - Inventory summary for low-stock books
- **Agent orchestration**: executes tools based on user prompts.

---

## ğŸ’¡ Example: Streamlit App in Action

Once you run the Streamlit app, youâ€™ll see an interface like this:
![RAG PDF Q&A Example](assets/app_example.png)

## Requirements

- Python 3.10+
- Libraries (can be installed via `requirements.txt`):
- [SQLite3](https://www.sqlite.org/index.html) â€“ For storing messages, tool calls, books, and orders
- [Ollama 3.1](https://ollama.com/) â€“ LLM model used via LangChain Ollama integration

```bash
pip install -r requirements.txt
```
---

## Project Structure
```
.
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api.py                  # FastAPI endpoints
â”‚   â”œâ”€â”€ api_helper_functions.py # Agent helper functions (store messages, call agent)
â”‚   â”œâ”€â”€ agent.py                # LLM and tools setup
â”‚   â”œâ”€â”€ db.py                   # Database query and execution functions
â”‚   â””â”€â”€ tools.py                # Bookstore business logic functions
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ bookstore_prompt.txt    # System prompt for agent
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ library.db              # SQLite database
â”‚   â”œâ”€â”€ schema.sql              # DB schema definitions
â”‚   â””â”€â”€ seed.sql                # Seed data for initial testing
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py                  # Streamlit frontend
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore file
â””â”€â”€ README.md                   # Project README
```
---

## Project Setup
### 1. Clone the repository

```bash
git clone https://github.com/Hashem-Qaryouti/llama3-rag-pipeline.git
cd <your-repo-name>
```
### 2. Create and activate a virtual environment
```bash
python -m venv <your-venv-name>
```
#### Activate the environment:
- **Windows:**
```bash
<your-venv-name>\Scripts\activate
```
- **Linux:**
```bash
source <your-venv-name>/bin/activate
```
### 3. Install dependencies
```bash
pip install -r requirements.txt

```

### 4. Set up the Database 
1. Create the SQLite database file (if not already created):

```bash
touch db/library.db
```
2. Initialize the database schema:
 ```bash
sqlite3 db/library.db < db/schema.sql
```
3. Seed the database with initial data:
 ```bash
sqlite3 db/library.db < db/seed.sql
```

### 5. Pull the Ollama Model
```bash
ollama pull llama3.1
```
---

## Usage
1. Run the Backend API
```bash
uvicorn server.api:app --reload --host 0.0.0.0 --port 8000
```
1. Run the Streamlit App
```bash
cd src
streamlit run app/app.py
```
- **Open the URL shown in your terminal (usually http://localhost:8501):**
- **Enter your question in the text box and click Send**
